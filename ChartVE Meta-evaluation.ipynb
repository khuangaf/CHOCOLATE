{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7dd9757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import requests\n",
    "from scipy.stats import kendalltau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "769f6307",
   "metadata": {},
   "outputs": [],
   "source": [
    "FACTUAL_ERROR_TYPES = ['label_error', 'magnitude_error', 'ooc_error', 'trend_error','value_error','nonsense_error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b8ea82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"khhuang/chartve\"\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_name).cuda()\n",
    "processor = DonutProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "666c73ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/chocolate.json\", \"r\") as f:\n",
    "    chocolate = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3efddfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_query(sentence):\n",
    "    return f\"Does the image entails this statement: \\\"{sentence}\\\"?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "344518cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proccess_samples(samples): \n",
    "    processed = []\n",
    "    for sample in samples:\n",
    "        img_id = '-'.join(sample['_id'].split('-')[1:]).replace('pew_','').replace('vistext_','') \n",
    "        caption_label = all([ label not in FACTUAL_ERROR_TYPES for sent_labels in sample[\"labels\"] for label in sent_labels]) \n",
    "        caption_label = int(caption_label)\n",
    "\n",
    "        for sentence, sent_labels in zip(sample[\"sentences\"], sample[\"labels\"]):\n",
    "#             image_path = sample[\"image_path\"]\n",
    "            if sample['dataset'] == 'pew':\n",
    "                try:\n",
    "                    table_type, image_id = img_id.replace('.txt','').split('_col-')\n",
    "                except:\n",
    "                    table_type, image_id = img_id.replace('.txt','').split('_col_')\n",
    "                if table_type.startswith('two'):\n",
    "                    root_image_path = \"/shared/nas/data/m1/khhuang3/mfec/data/Chart-to-text/pew_dataset/dataset/\"\n",
    "                else:\n",
    "                    root_image_path = \"/shared/nas/data/m1/khhuang3/mfec/data/Chart-to-text/pew_dataset/dataset/multiColumn\"\n",
    "\n",
    "                image_path=f\"{root_image_path}/imgs/{image_id}.png\"\n",
    "\n",
    "            else:\n",
    "                image_path = f\"/shared/nas/data/m1/khhuang3/mfec/data/vistext/data/images/{img_id}.png\"\n",
    "            \n",
    "            query = format_query(sentence)\n",
    "            sent_label = 0 if any([l in FACTUAL_ERROR_TYPES for l in sent_labels]) else 1\n",
    "            prompt =  \"<chartqa>  \" + query + \" <s_answer>\" \n",
    "            row = [sample['_id'], image_path, prompt, sent_label, caption_label]\n",
    "            processed.append(row)\n",
    "    processed = pd.DataFrame(processed, columns=['_id','image_path','prompt','sent_label','caption_label'])\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da0f358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(processed_df):\n",
    "    binary_positive_probs = []\n",
    "    with torch.no_grad():\n",
    "        for row in tqdm(processed_df.itertuples(), total=len(processed_df)):\n",
    "#             img = Image.open(requests.get(row.image_path, stream=True).raw)\n",
    "            img = Image.open(row.image_path)\n",
    "            pixel_values = processor(img.convert(\"RGB\"), random_padding=False, return_tensors=\"pt\").pixel_values\n",
    "            pixel_values = pixel_values.cuda()\n",
    "            decoder_input_ids = processor.tokenizer(row.prompt, add_special_tokens=False, return_tensors=\"pt\", max_length=510).input_ids.cuda()#.squeeze(0)\n",
    "            \n",
    "                    \n",
    "\n",
    "            outputs = model(pixel_values,\n",
    "                                     decoder_input_ids=decoder_input_ids)\n",
    "\n",
    "            # positive_logit = outputs['logits'].squeeze()[-1,49922]\n",
    "            # negative_logit = outputs['logits'].squeeze()[-1,2334] \n",
    "            \n",
    "            binary_entail_prob = torch.nn.functional.softmax(outputs['logits'].squeeze()[-1,[2334, 49922]])[1]\n",
    "            binary_positive_probs.append(binary_entail_prob.item())\n",
    "\n",
    "    processed_df['binary_entailment_prob'] = binary_positive_probs\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7f57c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(sample_id):\n",
    "    if \"bard\" in sample_id or \"gpt4v\" in sample_id:\n",
    "        return \"LVLM\"\n",
    "    elif \"deplot\" in sample_id:\n",
    "        return \"LLM\"\n",
    "    else:\n",
    "        return \"FT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cf2bc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_chocolate = proccess_samples(chocolate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6cf1868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31694/3235699514.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for row in tqdm(processed_df.itertuples(), total=len(processed_df)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4d7aa0dee22454597eac14b8bf5a0b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5323 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/tmp/ipykernel_31694/3235699514.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  binary_entail_prob = torch.nn.functional.softmax(outputs['logits'].squeeze()[-1,[2334, 49922]])[1]\n"
     ]
    }
   ],
   "source": [
    "processed_chocolate = get_prediction(processed_chocolate)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d834e111",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_chocolate[\"split\"] = processed_chocolate[\"_id\"].apply(get_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c1416f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2score = processed_chocolate.groupby('_id').binary_entailment_prob.min().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "673158f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_chocolate[\"chartve_score\"] = processed_chocolate['_id'].map(id2score)\n",
    "final_df = processed_chocolate.drop_duplicates('_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "557b8dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split LVLM| Tau: 0.178\n",
      "Split LLM| Tau: 0.091\n",
      "Split FT| Tau: 0.215\n"
     ]
    }
   ],
   "source": [
    "for split in ['LVLM','LLM','FT']:\n",
    "    current_df = final_df.loc[final_df.split == split].dropna()\n",
    "    tau = kendalltau(current_df.caption_label.values, current_df.chartve_score.values, variant='c').statistic\n",
    "    print(f\"Split {split}| Tau: {tau:.03f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
